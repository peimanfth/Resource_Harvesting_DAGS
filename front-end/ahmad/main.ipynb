{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 37\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# def generate_dataset(num_samples):\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     samples = []\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     for _ in range(num_samples):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Generate dataset\u001b[39;00m\n\u001b[1;32m     36\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400000\u001b[39m  \u001b[38;5;66;03m# You can adjust the number of samples\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m dataset\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[0;34m(num_samples)\u001b[0m\n\u001b[1;32m     20\u001b[0m     num_processes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;241m*\u001b[39mnum_processes_range)\n\u001b[1;32m     21\u001b[0m     size \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;241m*\u001b[39msize_range)\n\u001b[0;32m---> 22\u001b[0m     samples\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnormalize_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(samples)\n",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m, in \u001b[0;36mnormalize_params\u001b[0;34m(iteration, num_processes, size)\u001b[0m\n\u001b[1;32m     10\u001b[0m normalized_iteration \u001b[38;5;241m=\u001b[39m (iteration \u001b[38;5;241m-\u001b[39m iteration_range[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m (iteration_range[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m iteration_range[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     11\u001b[0m normalized_num_processes \u001b[38;5;241m=\u001b[39m (num_processes \u001b[38;5;241m-\u001b[39m num_processes_range[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m (num_processes_range[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m num_processes_range[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m normalized_size \u001b[38;5;241m=\u001b[39m (size \u001b[38;5;241m-\u001b[39m size_range[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m (size_range[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[43msize_range\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [normalized_iteration, normalized_num_processes, normalized_size]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters range\n",
    "iteration_range = (5000, 120000)\n",
    "num_processes_range = (1, 16)\n",
    "size_range = (50, 500)\n",
    "\n",
    "# Normalization function\n",
    "def normalize_params(iteration, num_processes, size):\n",
    "    normalized_iteration = (iteration - iteration_range[0]) / (iteration_range[1] - iteration_range[0])\n",
    "    normalized_num_processes = (num_processes - num_processes_range[0]) / (num_processes_range[1] - num_processes_range[0])\n",
    "    normalized_size = (size - size_range[0]) / (size_range[1] - size_range[0])\n",
    "    return [normalized_iteration, normalized_num_processes, normalized_size]\n",
    "\n",
    "# Generate dataset\n",
    "def generate_dataset(num_samples):\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        iteration = np.random.randint(*iteration_range)\n",
    "        num_processes = np.random.choice(*num_processes_range)\n",
    "        size = np.random.choice(*size_range)\n",
    "        samples.append(normalize_params(iteration, num_processes, size))\n",
    "    return np.array(samples)\n",
    "\n",
    "# def generate_dataset(num_samples):\n",
    "#     samples = []\n",
    "#     for _ in range(num_samples):\n",
    "#         iteration = np.random.randint(*iteration_range)\n",
    "#         num_processes = np.random.choice(num_processes_options)\n",
    "#         size = np.random.choice(size_options)\n",
    "#         samples.append([iteration, num_processes, size])\n",
    "#     return np.array(samples)\n",
    "\n",
    "\n",
    "# Generate dataset\n",
    "num_samples = 400000  # You can adjust the number of samples\n",
    "dataset = generate_dataset(num_samples)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7359"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.randint(*iteration_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converting to PyTorch tensors and moving to GPU if available\n",
    "train_tensor = torch.Tensor(train_data).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_tensor = torch.Tensor(test_data).to('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming inputs are already normalized to a 0-1 range for simplicity\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(3, 128), # Input layer: 3 inputs\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), # Hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3), # Output layer: Encoded form\n",
    "            nn.Sigmoid() # Ensuring output is in the 0-1 range\n",
    "            # nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(3, 128), # Input layer: Encoded form\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), # Hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3), # Output layer: Predicting original inputs\n",
    "            nn.Sigmoid() # Assuming the original inputs were normalized\n",
    "            # nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "decoder = Decoder().to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Encoder Loss: -0.0002053499699337408, Train Decoder Loss: 0.00020535899156238883, Test Loss: 0.10529646277427673\n",
      "Epoch 10, Train Encoder Loss: -0.00020531660446431488, Train Decoder Loss: 0.00020533443870954216, Test Loss: 0.10516113042831421\n",
      "Epoch 20, Train Encoder Loss: -0.0002053308806847781, Train Decoder Loss: 0.00020533473554532975, Test Loss: 0.10526196658611298\n",
      "Epoch 30, Train Encoder Loss: -0.00020533572835847736, Train Decoder Loss: 0.00020533722250256687, Test Loss: 0.10525839775800705\n",
      "Epoch 40, Train Encoder Loss: -0.0002053348695859313, Train Decoder Loss: 0.0002053358337841928, Test Loss: 0.1052589863538742\n",
      "Epoch 50, Train Encoder Loss: -0.0002053348797140643, Train Decoder Loss: 0.00020533597036264837, Test Loss: 0.10525906831026077\n",
      "Epoch 60, Train Encoder Loss: -0.00020533458802383394, Train Decoder Loss: 0.00020533536872826516, Test Loss: 0.10525902360677719\n",
      "Epoch 70, Train Encoder Loss: -0.0002053344988496974, Train Decoder Loss: 0.00020533518367446958, Test Loss: 0.10525918006896973\n",
      "Epoch 80, Train Encoder Loss: -0.0002053344043670222, Train Decoder Loss: 0.00020533503380138428, Test Loss: 0.10525916516780853\n",
      "Epoch 90, Train Encoder Loss: -0.00020533434876706451, Train Decoder Loss: 0.00020533493997063488, Test Loss: 0.10525917261838913\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "batch_size = 512\n",
    "epochs = 100\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss_encoder, train_loss_decoder = 0, 0\n",
    "    for i in range(0, len(train_tensor), batch_size):\n",
    "        batch = train_tensor[i:i+batch_size]\n",
    "\n",
    "        # Train Decoder (Adversary) to predict original inputs from the encoded forms\n",
    "        encoded = encoder(batch).detach()  # Detach to prevent gradients from flowing into the encoder\n",
    "        decoded = decoder(encoded)\n",
    "        decoder_loss = criterion(decoded, batch)\n",
    "\n",
    "        decoder_optimizer.zero_grad()\n",
    "        decoder_loss.backward()\n",
    "        decoder_optimizer.step()\n",
    "        train_loss_decoder += decoder_loss.item()\n",
    "\n",
    "        # Train Encoder to obfuscate the input against the Decoder\n",
    "        encoded = encoder(batch)\n",
    "        decoded = decoder(encoded)\n",
    "        encoder_loss = -criterion(decoded, batch)  # Using negative loss as an adversarial goal\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        encoder_loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        train_loss_encoder += encoder_loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_test = encoder(test_tensor)\n",
    "        decoded_test = decoder(encoded_test)\n",
    "        test_loss = criterion(decoded_test, test_tensor).item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Train Encoder Loss: {train_loss_encoder / len(train_tensor)}, Train Decoder Loss: {train_loss_decoder / len(train_tensor)}, Test Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2740, 0.1429, 0.5000],\n",
       "        [0.7104, 0.1429, 0.5000],\n",
       "        [0.6314, 1.0000, 0.2500],\n",
       "        ...,\n",
       "        [0.0125, 1.0000, 0.2500],\n",
       "        [0.8585, 0.5714, 1.0000],\n",
       "        [0.4269, 0.8571, 1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2500, 1.0000], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([0.1, 0.25, 1]).to('cuda')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 4.5737e-10, 1.0000e+00], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4990, 0.4995, 0.5003], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(encoder(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
